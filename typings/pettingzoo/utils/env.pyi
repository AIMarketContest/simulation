"""
This type stub file was generated by pyright.
"""

class AECEnv:
    """
    The AECEnv steps agents one at a time. If you are unsure if you
    have implemented a AECEnv correctly, try running the `api_test` documented in
    the Developer documentation on the website.
    """

    def __init__(self) -> None: ...
    def step(self, action):
        """
        Receives a dictionary of actions keyed by the agent name.
        Returns the observation dictionary, reward dictionary, done dictionary, and info dictionary,
        where each dictionary is keyed by the agent.
        """
        ...
    def reset(self):
        """
        Resets the environment to a starting state.
        """
        ...
    def seed(self, seed=...):  # -> None:
        """
        Reseeds the environment (making the resulting environment deterministic).
        `reset()` must be called after `seed()`, and before `step()`.
        """
        ...
    def observe(self, agent):
        """
        Returns the observation an agent currently can make. `last()` calls this function.
        """
        ...
    def render(self, mode=...):
        """
        Displays a rendered frame from the environment, if supported.
        Alternate render modes in the default environments are `'rgb_array'`
        which returns a numpy array and is supported by all environments outside of classic,
        and `'ansi'` which returns the strings printed (specific to classic environments).
        """
        ...
    def state(self):
        """
        State returns a global view of the environment appropriate for
        centralized training decentralized execution methods like QMIX
        """
        ...
    def close(self):  # -> None:
        """
        Closes the rendering window, subprocesses, network connections, or any other resources
        that should be released.
        """
        ...
    def observation_space(self, agent):
        """
        Takes in agent and returns the observation space for that agent.

        MUST return the same value for the same agent name

        Default implementation is to return the observation_spaces dict
        """
        ...
    def action_space(self, agent):
        """
        Takes in agent and returns the action space for that agent.

        MUST return the same value for the same agent name

        Default implementation is to return the action_spaces dict
        """
        ...
    @property
    def num_agents(self): ...
    @property
    def max_num_agents(self): ...
    def agent_iter(self, max_iter=...):  # -> AECIterable:
        """
        yields the current agent (self.agent_selection) when used in a loop where you step() each iteration.
        """
        ...
    def last(self, observe=...):  # -> tuple[Unknown | None, Unknown, Unknown, Unknown]:
        """
        returns observation, cumulative reward, done, info   for the current agent (specified by self.agent_selection)
        """
        ...
    def __str__(self) -> str:
        """
        returns a name which looks like: "space_invaders_v1"
        """
        ...
    @property
    def unwrapped(self): ...

class AECIterable:
    def __init__(self, env, max_iter) -> None: ...
    def __iter__(self): ...

class AECIterator:
    def __init__(self, env, max_iter) -> None: ...
    def __next__(self): ...

class ParallelEnv:
    """
    The Parallel environment steps every live agent at once. If you are unsure if you
    have implemented a ParallelEnv correctly, try running the `parallel_api_test` in
    the Developer documentation on the website.
    """

    def reset(self):
        """
        resets the environment and returns a dictionary of observations (keyed by the agent name)
        """
        ...
    def seed(self, seed=...):  # -> None:
        """
        Reseeds the environment (making it deterministic).
        `reset()` must be called after `seed()`, and before `step()`.
        """
        ...
    def step(self, actions):
        """
        receives a dictionary of actions keyed by the agent name.
        Returns the observation dictionary, reward dictionary, done dictionary,
        and info dictionary, where each dictionary is keyed by the agent.
        """
        ...
    def render(self, mode=...):
        """
        Displays a rendered frame from the environment, if supported.
        Alternate render modes in the default environments are `'rgb_array'`
        which returns a numpy array and is supported by all environments outside
        of classic, and `'ansi'` which returns the strings printed
        (specific to classic environments).
        """
        ...
    def close(self):  # -> None:
        """
        Closes the rendering window.
        """
        ...
    def state(self):
        """
        State returns a global view of the environment appropriate for
        centralized training decentralized execution methods like QMIX
        """
        ...
    def observation_space(self, agent):
        """
        Takes in agent and returns the observation space for that agent.

        MUST return the same value for the same agent name

        Default implementation is to return the observation_spaces dict
        """
        ...
    def action_space(self, agent):
        """
        Takes in agent and returns the action space for that agent.

        MUST return the same value for the same agent name

        Default implementation is to return the action_spaces dict
        """
        ...
    @property
    def num_agents(self): ...
    @property
    def max_num_agents(self): ...
    def __str__(self) -> str:
        """
        returns a name which looks like: "space_invaders_v1" by default
        """
        ...
    @property
    def unwrapped(self): ...
